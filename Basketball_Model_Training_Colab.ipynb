{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# College Basketball Prediction Model Training\n",
    "## GPU-Accelerated Training on Google Colab\n",
    "\n",
    "This notebook trains deep learning models for college basketball game predictions using historical data.\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- Runtime → Change runtime type → GPU (T4 or better)\n",
    "\n",
    "**Steps:**\n",
    "1. Upload your training data\n",
    "2. Configure training parameters\n",
    "3. Train models on GPU\n",
    "4. Download trained models\n",
    "5. Evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload your `basketball_training_data.csv` file generated from the data preparation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your basketball_training_data.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the data\n",
    "data_file = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(f\"\\nData loaded: {len(df)} games\")\n",
    "print(f\"Features: {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (customize based on your data)\n",
    "feature_columns = [\n",
    "    'home_offensive_efficiency', 'home_defensive_efficiency', 'home_tempo',\n",
    "    'away_offensive_efficiency', 'away_defensive_efficiency', 'away_tempo',\n",
    "    'home_kenpom_rating', 'away_kenpom_rating',\n",
    "    'home_recent_form', 'away_recent_form',\n",
    "    'neutral_site', 'tournament_game',\n",
    "    'home_wins', 'home_losses', 'away_wins', 'away_losses',\n",
    "    'home_avg_points', 'away_avg_points',\n",
    "    'home_avg_points_allowed', 'away_avg_points_allowed'\n",
    "]\n",
    "\n",
    "# Target columns\n",
    "target_columns = [\n",
    "    'home_won',  # Binary: 1 if home won, 0 if away won\n",
    "    'point_differential',  # Home points - Away points\n",
    "    'total_points'  # Total combined points\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "available_features = [col for col in feature_columns if col in df.columns]\n",
    "available_targets = [col for col in target_columns if col in df.columns]\n",
    "\n",
    "print(f\"Using {len(available_features)} features\")\n",
    "print(f\"Predicting {len(available_targets)} targets\")\n",
    "\n",
    "# Prepare data\n",
    "X = df[available_features].values\n",
    "y = df[available_targets].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler parameters\n",
    "scaler_params = {\n",
    "    'mean': scaler.mean_.tolist(),\n",
    "    'scale': scaler.scale_.tolist(),\n",
    "    'feature_names': available_features\n",
    "}\n",
    "\n",
    "with open('scaler_params.json', 'w') as f:\n",
    "    json.dump(scaler_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BasketballDataset(X_train_scaled, y_train)\n",
    "val_dataset = BasketballDataset(X_val_scaled, y_val)\n",
    "test_dataset = BasketballDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballPredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout=0.3):\n",
    "        super(BasketballPredictionModel, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output heads for different predictions\n",
    "        self.win_predictor = nn.Sequential(\n",
    "            nn.Linear(prev_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Win probability (0-1)\n",
    "        )\n",
    "        \n",
    "        self.margin_predictor = nn.Sequential(\n",
    "            nn.Linear(prev_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Point differential\n",
    "        )\n",
    "        \n",
    "        self.total_predictor = nn.Sequential(\n",
    "            nn.Linear(prev_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Total points\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        win_prob = self.win_predictor(features)\n",
    "        margin = self.margin_predictor(features)\n",
    "        total = self.total_predictor(features)\n",
    "        return torch.cat([win_prob, margin, total], dim=1)\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train_scaled.shape[1]\n",
    "model = BasketballPredictionModel(input_size).to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "bce_loss = nn.BCELoss()  # For win probability\n",
    "mse_loss = nn.MSELoss()  # For margin and total\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "early_stop_patience = 15\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Loss weights for multi-task learning\n",
    "loss_weights = {\n",
    "    'win': 2.0,      # Most important for betting\n",
    "    'margin': 1.5,   # Important for spread betting\n",
    "    'total': 1.0     # Important for totals betting\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Early stopping patience: {early_stop_patience}\")\n",
    "print(f\"  Loss weights: {loss_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        \n",
    "        # Calculate individual losses\n",
    "        win_loss = bce_loss(predictions[:, 0], y_batch[:, 0])\n",
    "        margin_loss = mse_loss(predictions[:, 1], y_batch[:, 1])\n",
    "        total_loss_val = mse_loss(predictions[:, 2], y_batch[:, 2])\n",
    "        \n",
    "        # Weighted combination\n",
    "        loss = (loss_weights['win'] * win_loss + \n",
    "                loss_weights['margin'] * margin_loss + \n",
    "                loss_weights['total'] * total_loss_val)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            win_loss = bce_loss(predictions[:, 0], y_batch[:, 0])\n",
    "            margin_loss = mse_loss(predictions[:, 1], y_batch[:, 1])\n",
    "            total_loss_val = mse_loss(predictions[:, 2], y_batch[:, 2])\n",
    "            \n",
    "            loss = (loss_weights['win'] * win_loss + \n",
    "                    loss_weights['margin'] * margin_loss + \n",
    "                    loss_weights['total'] * total_loss_val)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'history': history\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"  ✓ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[1].plot(history['learning_rates'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "predictions_list = []\n",
    "actuals_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        predictions = model(X_batch).cpu().numpy()\n",
    "        predictions_list.append(predictions)\n",
    "        actuals_list.append(y_batch.numpy())\n",
    "\n",
    "predictions = np.vstack(predictions_list)\n",
    "actuals = np.vstack(actuals_list)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Win prediction accuracy\n",
    "win_predictions = (predictions[:, 0] > 0.5).astype(int)\n",
    "win_actuals = actuals[:, 0].astype(int)\n",
    "win_accuracy = accuracy_score(win_actuals, win_predictions)\n",
    "\n",
    "# Margin prediction\n",
    "margin_mae = mean_absolute_error(actuals[:, 1], predictions[:, 1])\n",
    "margin_rmse = np.sqrt(mean_squared_error(actuals[:, 1], predictions[:, 1]))\n",
    "\n",
    "# Total prediction\n",
    "total_mae = mean_absolute_error(actuals[:, 2], predictions[:, 2])\n",
    "total_rmse = np.sqrt(mean_squared_error(actuals[:, 2], predictions[:, 2]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nWin Prediction:\")\n",
    "print(f\"  Accuracy: {win_accuracy:.1%}\")\n",
    "print(f\"\\nPoint Differential:\")\n",
    "print(f\"  MAE: {margin_mae:.2f} points\")\n",
    "print(f\"  RMSE: {margin_rmse:.2f} points\")\n",
    "print(f\"\\nTotal Points:\")\n",
    "print(f\"  MAE: {total_mae:.2f} points\")\n",
    "print(f\"  RMSE: {total_rmse:.2f} points\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'win_accuracy': float(win_accuracy),\n",
    "    'margin_mae': float(margin_mae),\n",
    "    'margin_rmse': float(margin_rmse),\n",
    "    'total_mae': float(total_mae),\n",
    "    'total_rmse': float(total_rmse),\n",
    "    'test_samples': len(predictions)\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for deployment\n",
    "import zipfile\n",
    "\n",
    "# Create zip file with all necessary files\n",
    "with zipfile.ZipFile('basketball_model_package.zip', 'w') as zipf:\n",
    "    zipf.write('best_model.pth')\n",
    "    zipf.write('scaler_params.json')\n",
    "    zipf.write('evaluation_results.json')\n",
    "    zipf.write('training_history.png')\n",
    "\n",
    "print(\"Model package created!\")\n",
    "print(\"\\nDownloading files...\")\n",
    "\n",
    "# Download the package\n",
    "files.download('basketball_model_package.zip')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Extract the zip file\")\n",
    "print(\"2. Move files to your project's models/ directory\")\n",
    "print(\"3. Use the deployment script to integrate the model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
