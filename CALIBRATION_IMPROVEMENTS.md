# Calibration and Performance Improvements

## Overview

This document describes the improvements made to the college basketball prediction system to address calibration issues and improve prediction accuracy, particularly for 50-60% confidence predictions.

## Improvements Implemented

### 1. Prediction Logging System ‚úÖ

**File:** `prediction_logger.py`

**Features:**
- Logs every prediction to `data/predictions/prediction_log.json`
- Tracks both raw and calibrated confidence scores
- Records complete prediction context (teams, odds, reasoning, factors)
- Updates with actual outcomes after games complete
- Generates performance statistics by confidence tier
- Thread-safe file locking to prevent data corruption

**Usage:**
```python
from prediction_logger import PredictionLogger

logger = PredictionLogger()

# Log a prediction
prediction_id = logger.log_prediction({
    'game_id': 'duke_unc_20251109',
    'game_date': '2025-11-09',
    'home_team': 'Duke',
    'away_team': 'UNC',
    'bet_type': 'spread',
    'raw_confidence': 0.85,
    'calibrated_confidence': 0.72,  # Applied 0.85x multiplier
    # ... other fields
})

# After game completes
logger.update_outcome(prediction_id, {
    'actual_result': 'Duke won by 5',
    'bet_outcome': 'WIN',
    'profit_loss': 0.91
})

# Get performance stats
stats = logger.get_performance_stats()
```

### 2. Confidence Calibration Formula ‚úÖ

**File:** `core_basketball_prediction_engine.py`

**Implementation:**
- Applies 0.85x multiplier to any confidence over 80%
- Prevents overconfident predictions that don't reflect true win rates
- Logs calibration adjustments for transparency

**Example:**
- Raw confidence: 87% ‚Üí Calibrated: 74% (87% √ó 0.85)
- Raw confidence: 92% ‚Üí Calibrated: 78% (92% √ó 0.85)
- Raw confidence: 75% ‚Üí No change (below 80% threshold)

**Settings (configurable):**
```python
self.CALIBRATION_THRESHOLD = 0.80  # Apply above 80%
self.CALIBRATION_MULTIPLIER = 0.85  # Multiply by 0.85
```

### 3. Problematic Confidence Bucket Filtering ‚úÖ

**File:** `core_basketball_prediction_engine.py`

**Implementation:**
- Flags predictions in 50-60% confidence range
- Adds warning to risk factors
- Logs prominently for review
- Suggests avoiding bets or retraining model

**Warning Output:**
```
‚ö†Ô∏è  Prediction in problematic 50-60% confidence bucket (54.3%). 
Consider avoiding bet or retraining model.
```

**Risk Factor Added:**
```
‚ö†Ô∏è Problematic 50-60% confidence bucket
```

### 4. RAG System for Referee & Historical Data ‚úÖ

**File:** `basketball_rag_system.py`

**Features:**
- **Referee Profiles:** Tracks foul tendencies, home/away bias, pace impact
- **Historical Matchups:** Head-to-head records, scoring trends, venue splits
- **Venue Profiles:** Home court advantage, upset frequency, crowd impact
- **Contextual Insights:** Automatically generates relevant insights for predictions

**Data Tracked:**

#### Referee Tendencies
- Average fouls per game
- Home team foul bias (¬±)
- Pace factor (game tempo impact)
- Technical foul rate
- Conference specialty
- Tournament experience

#### Historical Matchups
- Win/loss records
- Average point differentials
- Scoring trends (high/low scoring)
- Recent form (last 3-5 games)
- Neutral site vs home/away splits
- Tournament history

#### Venue Characteristics
- Home win percentage
- Average total points
- Upset frequency
- Crowd impact factors
- Venue-specific adjustments

**Usage:**
```python
from basketball_rag_system import BasketballRAGSystem

rag = BasketballRAGSystem()

# Get comprehensive context
context = rag.get_comprehensive_context(
    team_a="Duke",
    team_b="North Carolina",
    venue="Cameron Indoor Stadium",
    referee="John Higgins"
)

# Returns contextual insights like:
# - "Duke won last 3 meetings"
# - "üè† Strong home court advantage at Cameron Indoor Stadium (84.2%)"
# - "üü° Referee John Higgins has home team foul bias"
# - "üê¢ Referee slows game pace"
```

## Integration with Core Engine

The improvements are integrated into `core_basketball_prediction_engine.py`:

1. **Initialization:** Prediction logger and RAG system initialized on engine startup
2. **Calibration:** Applied during confidence calculation in `_generate_betting_recommendations()`
3. **Logging:** Every betting recommendation automatically logged
4. **RAG Context:** Available for future integration into prediction adjustments

## Performance Tracking

### Metrics Tracked
- **Win Rate by Confidence Tier:** HIGH (75%+), MEDIUM (60-75%), LOW (<60%)
- **ROI by Confidence Tier:** Return on investment for each confidence level
- **Calibration Accuracy:** How well predicted confidence matches actual win rates
- **Problematic Bucket Performance:** Specific tracking of 50-60% predictions

### Accessing Performance Data

```python
from prediction_logger import PredictionLogger

logger = PredictionLogger()
stats = logger.get_performance_stats()

print(f"Overall Win Rate: {stats['win_rate']:.1%}")
print(f"ROI: {stats['roi']:.1f}%")
print(f"High Confidence: {stats['by_confidence']['HIGH']['win_rate']:.1%}")
print(f"Medium Confidence: {stats['by_confidence']['MEDIUM']['win_rate']:.1%}")
print(f"Low Confidence: {stats['by_confidence']['LOW']['win_rate']:.1%}")
```

## Next Steps & Recommendations

### Short Term
1. **Start Logging:** Begin logging all predictions immediately to build historical data
2. **Monitor Calibration:** Track whether 80%+ predictions are now properly calibrated
3. **Avoid 50-60% Bucket:** Skip betting on predictions flagged as problematic until retraining

### Medium Term
4. **Retrain Base Model:** Focus on improving 50-60% prediction accuracy
   - Collect more features for close games
   - Add RAG context to training data
   - Consider ensemble methods for uncertain predictions
   
5. **Integrate RAG Deeply:** Use referee/historical context as model features
   - Adjust tempo predictions based on referee pace factor
   - Weight historical matchups in close predictions
   - Apply venue-specific adjustments automatically

### Long Term
6. **Calibration Analysis:** After 50-100 logged predictions, analyze:
   - Are 75% confidence predictions winning 75% of the time?
   - Does the 0.85 multiplier need adjustment?
   - Which confidence buckets are most profitable?

7. **Model Retraining:** Use logged predictions to retrain:
   - Focus on improving 50-60% bucket
   - Add RAG features to training pipeline
   - Implement separate models for different game types (tournament vs regular season)

## Files Modified/Created

### New Files
- `prediction_logger.py` - Prediction logging system
- `basketball_rag_system.py` - RAG system for contextual data
- `CALIBRATION_IMPROVEMENTS.md` - This document

### Modified Files
- `core_basketball_prediction_engine.py` - Added calibration, filtering, and logging integration

### Dependencies
Install if not already available:
```bash
pip install filelock  # For thread-safe file operations
```

## Testing

Run the test scripts to verify functionality:

```bash
# Test prediction logger
python prediction_logger.py

# Test RAG system
python basketball_rag_system.py

# Test core engine (with logging)
python core_basketball_prediction_engine.py
```

## Configuration

### Adjust Calibration Settings

In `core_basketball_prediction_engine.py`:

```python
# Increase/decrease threshold
self.CALIBRATION_THRESHOLD = 0.80  # Default: 80%

# Adjust multiplier strength
self.CALIBRATION_MULTIPLIER = 0.85  # Default: 0.85 (85%)

# Change problematic bucket range
self.PROBLEMATIC_CONFIDENCE_MIN = 0.50  # Default: 50%
self.PROBLEMATIC_CONFIDENCE_MAX = 0.60  # Default: 60%
```

### Customize Logging Path

```python
# Custom log location
logger = PredictionLogger(log_path="custom/path/predictions.json")

# Custom RAG database
rag = BasketballRAGSystem(db_path="custom/path/rag.db")
```

## Summary

These improvements provide:
1. ‚úÖ **Complete prediction tracking** - Every pick logged with context
2. ‚úÖ **Calibrated confidence** - Overconfident predictions adjusted automatically
3. ‚úÖ **Problematic bucket identification** - 50-60% predictions flagged for review
4. ‚úÖ **Rich contextual data** - Referee and historical insights for better decisions

The system is now production-ready and will automatically log, calibrate, and contextualize every prediction for continuous improvement and analysis.
