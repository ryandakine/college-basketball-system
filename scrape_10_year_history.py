#!/usr/bin/env python3
"""
üèÄ 10-YEAR HISTORICAL DATA SCRAPER
==================================

Scrapes comprehensive college basketball data from 2015-2024:
- ESPN: Game results, scores, schedules
- Sports Reference: Team stats, records
- Barttorvik: Historical efficiency ratings
- Covers: Historical betting lines (where available)

Total: ~50,000+ games over 10 seasons

WARNING: This takes 2-4 hours to run completely!
Use --quick for faster testing (1 season only)
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import sqlite3
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)


class TenYearHistoricalScraper:
    """
    Comprehensive historical data scraper for college basketball

    Data Sources:
    1. ESPN API - Game results, scores (FREE)
    2. Sports Reference - Team stats (FREE, rate limited)
    3. Barttorvik - Efficiency ratings (FREE)
    """

    def __init__(self, start_year: int = 2015, end_year: int = 2024):
        self.start_year = start_year
        self.end_year = end_year

        # Create output directory
        self.output_dir = Path('data/historical')
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Session with headers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        # API endpoints
        self.espn_base = "https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball"
        self.sports_ref_base = "https://www.sports-reference.com/cbb"
        self.barttorvik_base = "https://barttorvik.com"

        # Rate limiting
        self.request_delay = 1.0  # seconds between requests

        logger.info(f"üèÄ 10-Year Scraper initialized")
        logger.info(f"   Seasons: {start_year} to {end_year}")
        logger.info(f"   Output: {self.output_dir}")

    def scrape_all(self) -> Dict:
        """
        Scrape ALL historical data

        WARNING: This takes 2-4 hours!

        Returns:
            Summary of scraped data
        """
        logger.info("\n" + "="*70)
        logger.info("üèÄ STARTING 10-YEAR HISTORICAL SCRAPE")
        logger.info("   This will take 2-4 hours!")
        logger.info("="*70 + "\n")

        results = {
            'games': [],
            'team_stats': [],
            'efficiency': [],
            'errors': []
        }

        total_games = 0

        for year in range(self.start_year, self.end_year + 1):
            season_name = f"{year-1}-{str(year)[2:]}"
            logger.info(f"\nüìÖ Scraping {season_name} season...")

            # 1. Scrape ESPN games
            games = self.scrape_espn_season(year)
            results['games'].extend(games)
            total_games += len(games)
            logger.info(f"   ESPN Games: {len(games)}")

            # 2. Scrape Barttorvik efficiency
            efficiency = self.scrape_barttorvik_season(year)
            results['efficiency'].extend(efficiency)
            logger.info(f"   Barttorvik Teams: {len(efficiency)}")

            # Save yearly file
            self._save_season_data(year, games, efficiency)

            # Progress
            logger.info(f"   ‚úÖ {season_name} complete - Total games: {total_games}")

        # Save combined file
        self._save_combined_data(results)

        logger.info("\n" + "="*70)
        logger.info(f"‚úÖ SCRAPING COMPLETE!")
        logger.info(f"   Total Games: {total_games}")
        logger.info(f"   Seasons: {self.end_year - self.start_year + 1}")
        logger.info(f"   Data saved to: {self.output_dir}")
        logger.info("="*70 + "\n")

        return results

    def scrape_espn_season(self, year: int) -> List[Dict]:
        """
        Scrape all games for a season from ESPN

        Args:
            year: Season end year (e.g., 2024 for 2023-24 season)

        Returns:
            List of game dictionaries
        """
        games = []

        # College basketball season: November to April
        # Start from November of previous year
        start_date = datetime(year - 1, 11, 1)
        end_date = datetime(year, 4, 15)

        current_date = start_date

        while current_date <= end_date:
            date_str = current_date.strftime("%Y%m%d")

            try:
                # ESPN scoreboard API
                url = f"{self.espn_base}/scoreboard?dates={date_str}"
                response = self.session.get(url, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    day_games = self._parse_espn_scoreboard(data, year)
                    games.extend(day_games)

                    if day_games:
                        logger.debug(f"   {date_str}: {len(day_games)} games")

                time.sleep(self.request_delay)

            except Exception as e:
                logger.warning(f"   Error on {date_str}: {e}")

            current_date += timedelta(days=1)

        return games

    def _parse_espn_scoreboard(self, data: Dict, season_year: int) -> List[Dict]:
        """Parse ESPN scoreboard response"""
        games = []

        events = data.get('events', [])

        for event in events:
            try:
                competition = event.get('competitions', [{}])[0]
                competitors = competition.get('competitors', [])

                if len(competitors) != 2:
                    continue

                # Get teams
                home_team = None
                away_team = None
                home_score = None
                away_score = None

                for comp in competitors:
                    team_name = comp.get('team', {}).get('displayName', '')
                    score = int(comp.get('score', 0))

                    if comp.get('homeAway') == 'home':
                        home_team = team_name
                        home_score = score
                    else:
                        away_team = team_name
                        away_score = score

                if not home_team or not away_team:
                    continue

                # Only completed games
                status = event.get('status', {}).get('type', {}).get('completed', False)
                if not status:
                    continue

                game = {
                    'game_id': event.get('id', ''),
                    'game_date': event.get('date', '')[:10],
                    'season': season_year,
                    'home_team': home_team,
                    'away_team': away_team,
                    'home_score': home_score,
                    'away_score': away_score,
                    'winner': home_team if home_score > away_score else away_team,
                    'total_points': home_score + away_score,
                    'point_diff': abs(home_score - away_score),
                    'venue': competition.get('venue', {}).get('fullName', ''),
                    'attendance': competition.get('attendance', 0),
                    'neutral_site': competition.get('neutralSite', False),
                    'conference_game': competition.get('conferenceCompetition', False),
                    'tournament': 'NCAA' in event.get('name', '') or 'Tournament' in event.get('name', ''),
                    'data_source': 'ESPN'
                }

                games.append(game)

            except Exception as e:
                logger.debug(f"Error parsing game: {e}")
                continue

        return games

    def scrape_barttorvik_season(self, year: int) -> List[Dict]:
        """
        Scrape Barttorvik efficiency ratings for a season
        Falls back to Sports Reference if Barttorvik is blocked

        Args:
            year: Season end year

        Returns:
            List of team efficiency dictionaries
        """
        teams = []

        # Try Barttorvik first
        try:
            url = f"{self.barttorvik_base}/trank.php?year={year}"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200 and 'Access Denied' not in response.text:
                soup = BeautifulSoup(response.content, 'html.parser')

                # Find ratings table
                table = soup.find('table')
                if table:
                    rows = table.find_all('tr')[1:]  # Skip header

                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) < 10:
                            continue

                        try:
                            team_data = {
                                'season': year,
                                'rank': int(cols[0].text.strip()),
                                'team': cols[1].text.strip(),
                                'conference': cols[2].text.strip(),
                                'record': cols[3].text.strip(),
                                'adj_oe': float(cols[4].text.strip()),
                                'adj_de': float(cols[5].text.strip()),
                                'adj_em': float(cols[6].text.strip()),
                                'tempo': float(cols[7].text.strip()),
                                'barthag': float(cols[8].text.strip()) if cols[8].text.strip() else 0.5,
                                'data_source': 'BARTTORVIK'
                            }

                            # Parse record
                            record = cols[3].text.strip()
                            if '-' in record:
                                parts = record.split('-')
                                team_data['wins'] = int(parts[0])
                                team_data['losses'] = int(parts[1])

                            teams.append(team_data)

                        except (ValueError, IndexError) as e:
                            continue

                    time.sleep(self.request_delay)

                    if teams:
                        logger.info(f"   ‚úÖ Barttorvik: {len(teams)} teams")
                        return teams

            logger.warning(f"Barttorvik blocked for {year}, trying Sports Reference...")

        except Exception as e:
            logger.warning(f"Barttorvik error for {year}: {e}, trying Sports Reference...")

        # Fallback to Sports Reference
        teams = self.scrape_sports_reference_ratings(year)
        return teams

    def scrape_sports_reference_ratings(self, year: int) -> List[Dict]:
        """
        Scrape team ratings from Sports Reference as fallback

        Args:
            year: Season end year

        Returns:
            List of team efficiency dictionaries
        """
        teams = []

        try:
            # Sports Reference college basketball ratings page
            url = f"{self.sports_ref_base}/seasons/{year}-school-stats.html"
            response = self.session.get(url, timeout=15)

            if response.status_code != 200:
                logger.warning(f"Sports Reference returned {response.status_code} for {year}")
                return teams

            soup = BeautifulSoup(response.content, 'html.parser')

            # Find the main stats table
            table = soup.find('table', {'id': 'basic_school_stats'})
            if not table:
                # Try alternate table ID
                table = soup.find('table', {'class': 'stats_table'})

            if not table:
                logger.warning(f"No stats table found for {year}")
                return teams

            tbody = table.find('tbody')
            if not tbody:
                return teams

            rows = tbody.find_all('tr')
            rank = 0

            for row in rows:
                # Skip header rows
                if row.get('class') and 'thead' in row.get('class', []):
                    continue

                cols = row.find_all(['td', 'th'])
                if len(cols) < 10:
                    continue

                try:
                    rank += 1

                    # Extract team name
                    team_cell = row.find('td', {'data-stat': 'school_name'})
                    if not team_cell:
                        team_cell = cols[0]
                    team_name = team_cell.text.strip()

                    if not team_name:
                        continue

                    # Extract stats
                    def get_stat(stat_name, default=0):
                        cell = row.find('td', {'data-stat': stat_name})
                        if cell and cell.text.strip():
                            try:
                                return float(cell.text.strip())
                            except ValueError:
                                return default
                        return default

                    # Get basic stats
                    wins = int(get_stat('wins', 0))
                    losses = int(get_stat('losses', 0))
                    pts = get_stat('pts', 70)
                    opp_pts = get_stat('opp_pts', 70)

                    # Calculate efficiency metrics
                    games = wins + losses
                    if games > 0:
                        ppg = pts / games if pts > 10 else get_stat('pts_per_g', 70)
                        opp_ppg = opp_pts / games if opp_pts > 10 else get_stat('opp_pts_per_g', 70)
                    else:
                        ppg = get_stat('pts_per_g', 70)
                        opp_ppg = get_stat('opp_pts_per_g', 70)

                    # Estimate adjusted efficiency (not as accurate as Barttorvik but usable)
                    # Average D1 efficiency is ~100 for both offense and defense
                    adj_oe = ppg * 1.43  # Rough conversion to per-100-possessions
                    adj_de = opp_ppg * 1.43
                    adj_em = adj_oe - adj_de

                    # Estimate tempo and win probability
                    tempo = get_stat('pace', 68)
                    if tempo == 0:
                        tempo = 68  # D1 average

                    # Calculate Pythag win expectation
                    if ppg + opp_ppg > 0:
                        pythag = (ppg ** 11.5) / ((ppg ** 11.5) + (opp_ppg ** 11.5))
                    else:
                        pythag = 0.5

                    team_data = {
                        'season': year,
                        'rank': rank,
                        'team': team_name,
                        'conference': get_stat('conf_abbr', ''),
                        'record': f"{wins}-{losses}",
                        'wins': wins,
                        'losses': losses,
                        'adj_oe': round(adj_oe, 1),
                        'adj_de': round(adj_de, 1),
                        'adj_em': round(adj_em, 1),
                        'tempo': round(tempo, 1),
                        'barthag': round(pythag, 3),
                        'data_source': 'SPORTS_REFERENCE'
                    }

                    teams.append(team_data)

                except (ValueError, IndexError, AttributeError) as e:
                    continue

            # Sort by estimated efficiency margin
            teams.sort(key=lambda x: x.get('adj_em', 0), reverse=True)
            for i, team in enumerate(teams):
                team['rank'] = i + 1

            time.sleep(3.0)  # Sports Reference needs longer delays

            if teams:
                logger.info(f"   ‚úÖ Sports Reference: {len(teams)} teams")

        except Exception as e:
            logger.error(f"Error scraping Sports Reference {year}: {e}")

        return teams

    def scrape_sports_reference_team(self, team_slug: str, year: int) -> Optional[Dict]:
        """
        Scrape detailed team stats from Sports Reference

        Note: Sports Reference has strict rate limiting!
        """
        try:
            url = f"{self.sports_ref_base}/schools/{team_slug}/{year}.html"
            response = self.session.get(url, timeout=10)

            if response.status_code != 200:
                return None

            soup = BeautifulSoup(response.content, 'html.parser')

            # Parse team stats (structure varies)
            stats = {
                'team': team_slug,
                'season': year,
                'data_source': 'SPORTS_REFERENCE'
            }

            # Find stats tables and extract data
            # Note: Actual parsing depends on Sports Reference's HTML structure

            time.sleep(3.0)  # Sports Reference needs longer delays

            return stats

        except Exception as e:
            logger.debug(f"Sports Reference error for {team_slug}: {e}")
            return None

    def _save_season_data(self, year: int, games: List[Dict], efficiency: List[Dict]):
        """Save single season data"""
        season_dir = self.output_dir / str(year)
        season_dir.mkdir(exist_ok=True)

        # Games
        if games:
            games_df = pd.DataFrame(games)
            games_df.to_csv(season_dir / 'games.csv', index=False)

            with open(season_dir / 'games.json', 'w') as f:
                json.dump(games, f, indent=2)

        # Efficiency
        if efficiency:
            eff_df = pd.DataFrame(efficiency)
            eff_df.to_csv(season_dir / 'efficiency.csv', index=False)

            with open(season_dir / 'efficiency.json', 'w') as f:
                json.dump(efficiency, f, indent=2)

        logger.info(f"   üíæ Saved {year} data to {season_dir}")

    def _save_combined_data(self, results: Dict):
        """Save combined data from all seasons"""

        # Combined games
        if results['games']:
            all_games_df = pd.DataFrame(results['games'])
            all_games_df.to_csv(self.output_dir / 'all_games_10yr.csv', index=False)
            logger.info(f"üíæ Saved all_games_10yr.csv ({len(results['games'])} games)")

        # Combined efficiency
        if results['efficiency']:
            all_eff_df = pd.DataFrame(results['efficiency'])
            all_eff_df.to_csv(self.output_dir / 'all_efficiency_10yr.csv', index=False)
            logger.info(f"üíæ Saved all_efficiency_10yr.csv ({len(results['efficiency'])} team-seasons)")

        # Save to database
        self._save_to_database(results)

    def _save_to_database(self, results: Dict):
        """Save all data to SQLite database"""
        db_path = self.output_dir / 'historical_10yr.db'
        conn = sqlite3.connect(db_path)

        # Games table
        if results['games']:
            games_df = pd.DataFrame(results['games'])
            games_df.to_sql('historical_games', conn, if_exists='replace', index=False)

        # Efficiency table
        if results['efficiency']:
            eff_df = pd.DataFrame(results['efficiency'])
            eff_df.to_sql('historical_efficiency', conn, if_exists='replace', index=False)

        conn.close()
        logger.info(f"üíæ Saved to database: {db_path}")

    def quick_scrape(self, year: int = 2024) -> Dict:
        """
        Quick scrape of single season for testing

        Args:
            year: Season to scrape (default: 2024)

        Returns:
            Season data
        """
        logger.info(f"\nüèÄ Quick scrape: {year-1}-{str(year)[2:]} season only\n")

        games = self.scrape_espn_season(year)
        efficiency = self.scrape_barttorvik_season(year)

        self._save_season_data(year, games, efficiency)

        return {
            'games': games,
            'efficiency': efficiency,
            'season': year
        }


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="üèÄ 10-Year Historical Data Scraper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scrape_10_year_history.py              # Full 10-year scrape (2-4 hours)
  python scrape_10_year_history.py --quick      # Single season test (~10 min)
  python scrape_10_year_history.py --year 2023  # Specific season
        """
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick test with single season (2024)'
    )

    parser.add_argument(
        '--year',
        type=int,
        help='Scrape specific season only'
    )

    parser.add_argument(
        '--start',
        type=int,
        default=2015,
        help='Start year (default: 2015)'
    )

    parser.add_argument(
        '--end',
        type=int,
        default=2024,
        help='End year (default: 2024)'
    )

    args = parser.parse_args()

    print("\n" + "="*70)
    print("üèÄ 10-YEAR HISTORICAL DATA SCRAPER")
    print("   College Basketball Backtesting Data")
    print("="*70)

    scraper = TenYearHistoricalScraper(start_year=args.start, end_year=args.end)

    if args.quick:
        print("\n‚ö° QUICK MODE - Single season only (~10 minutes)\n")
        results = scraper.quick_scrape(2024)
        print(f"\n‚úÖ Quick scrape complete!")
        print(f"   Games: {len(results['games'])}")
        print(f"   Teams: {len(results['efficiency'])}")

    elif args.year:
        print(f"\nüìÖ Single season: {args.year-1}-{str(args.year)[2:]}\n")
        results = scraper.quick_scrape(args.year)
        print(f"\n‚úÖ Season scrape complete!")
        print(f"   Games: {len(results['games'])}")
        print(f"   Teams: {len(results['efficiency'])}")

    else:
        print(f"\n‚è≥ FULL 10-YEAR SCRAPE")
        print(f"   This will take 2-4 hours!")
        print(f"   Seasons: {args.start} to {args.end}")
        print("")

        confirm = input("Continue? (y/n): ").strip().lower()
        if confirm != 'y':
            print("Cancelled.")
            return

        results = scraper.scrape_all()
        print(f"\n‚úÖ Full scrape complete!")
        print(f"   Total Games: {len(results['games'])}")

    print(f"\nüìÅ Data saved to: data/historical/")
    print(f"   - all_games_10yr.csv")
    print(f"   - all_efficiency_10yr.csv")
    print(f"   - historical_10yr.db")
    print("")


if __name__ == "__main__":
    main()
